{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6c2028",
   "metadata": {},
   "source": [
    "###\n",
    "## 1) <u>Weights in Machine Learning</u>\n",
    "\n",
    "In machine learning, **weights** (also called **coefficients**) refer to the values that determine the importance or influence of each feature (input variable) in a model. They control how much impact each feature has on the model's predictions.\n",
    "\n",
    "#### Weights in Logistic Regression\n",
    "\n",
    "In **logistic regression**, weights are the parameters that the model learns during training. Each feature in your dataset has a corresponding weight. The model combines the features and their weights to calculate the final prediction.\n",
    "\n",
    "The general formula for logistic regression is:\n",
    "\n",
    "$$\n",
    "\\text{Prediction} = \\sigma(w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\dots + w_n \\cdot x_n + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- w1, w2, ..., wn are the weights for features x1, x2, ..., xn,\n",
    "- b is the bias (an additional term that adjusts the overall prediction),\n",
    "- σ is the sigmoid function, which transforms the output into a probability between 0 and 1.\n",
    "\n",
    "\n",
    "#### Example\n",
    "\n",
    "Suppose you are using logistic regression to predict whether a student passes (1) or fails (0) an exam based on their **study hours** and **sleep hours**. The model might assign:\n",
    "- A **weight** of 0.5 to study hours (meaning study hours have a moderate positive influence),\n",
    "- A **weight** of -0.2 to sleep hours (meaning too many sleep hours might reduce the likelihood of passing),\n",
    "- A **bias** of 0.1.\n",
    "\n",
    "The formula for prediction would be:\n",
    "\\[\n",
    "\\text{Prediction} = \\sigma(0.5 \\cdot \\text{study hours} + (-0.2) \\cdot \\text{sleep hours} + 0.1)\n",
    "\\]\n",
    "The output of this prediction will be a probability (between 0 and 1) indicating whether the student is likely to pass or fail.\n",
    "\n",
    "#### Interpretation of Weights\n",
    "- **Positive weights**: Increase the likelihood of the positive outcome (e.g., passing an exam).\n",
    "- **Negative weights**: Decrease the likelihood of the positive outcome (e.g., failing instead of passing).\n",
    "- **Larger weights**: Indicate stronger influence on the outcome, while smaller weights indicate weaker influence.\n",
    "\n",
    "During training, the model adjusts the weights to minimize the difference between the actual and predicted outputs, helping the model make accurate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27422b6",
   "metadata": {},
   "source": [
    "### \n",
    "## 2) <u>Rugularization</u>\n",
    "In models like logistic regression, the goal is to find the optimal weights for each feature. However, if the model puts too much importance on certain features (i.e., if the weights become very large), it might overfit the data.\n",
    "\n",
    "Regularization helps by adding a penalty to the model when the weights become too large, forcing the model to keep the weights smaller and simpler. This makes the model more general and better at predicting unseen data.\n",
    "\n",
    "#### Types of Regularization:\n",
    "**L1 regularization (Lasso):** Adds a penalty based on the absolute values of the weights, which can shrink some weights to zero, effectively removing less important features.\n",
    "\n",
    "**L2 regularization (Ridge):** Adds a penalty based on the squared values of the weights, which forces the weights to be smaller but doesn't reduce them to zero. what is meant by penalty\n",
    "\n",
    "#### Example\n",
    "\n",
    "If you have a model with 10 columns and the weights are:\n",
    "\n",
    "* Without Regularization: w1 = 50, w2 = 100, w3 = -150\n",
    "* With Regularization: w1 = 5, w2 = 7, w3 = -10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6790f6",
   "metadata": {},
   "source": [
    "###\n",
    "## 3) <u>Penalty</u>\n",
    "In the context of regularization, the penalty is a term added to the model’s loss function to constrain or penalize the complexity of the model. The penalty term discourages the model from assigning excessively large values to the weights (coefficients), which helps to prevent overfitting.\n",
    "\n",
    "#### L1 Regularization (Lasso) Example\n",
    "\n",
    "Let's illustrate how L1 regularization (Lasso) applies a penalty with a simplified example.\n",
    "\n",
    "#### Example Dataset\n",
    "\n",
    "Assume you have a dataset with 4 columns (features) and 10 rows. Let's say the weights (coefficients) learned by your model for these features are as follows:\n",
    "\n",
    "- \\( w1 = 3.5 \\)\n",
    "- \\( w2 = -2.0 \\)\n",
    "- \\( w3 = 0.0 \\)  (This weight could be zero due to L1 regularization)\n",
    "- \\( w4 = 1.5 \\)\n",
    "\n",
    "#### L1 Regularization Penalty Calculation\n",
    "\n",
    "L1 regularization adds a penalty term to the loss function based on the absolute values of the weights. The penalty term for L1 regularization is:\n",
    "\n",
    "Penalty_L1 = λ * Σ(i=1 to n) |w_i|\n",
    "\n",
    "Where:\n",
    "- \\( lambda \\) is the regularization parameter that controls the strength of the penalty.\n",
    "- \\( w_i \\) are the weights.\n",
    "\n",
    "Let’s calculate the penalty using a specific value for \\( \\lambda \\). Suppose \\( \\lambda = 0.1 \\).\n",
    "\n",
    "1. **Sum of Absolute Values of Weights**:\n",
    "\n",
    "\\[\n",
    "Σ(i=1 to 4) |w_i| = |3.5| + |-2.0| + |0.0| + |1.5| = 3.5 + 2.0 + 0.0 + 1.5 = 7.0\\]\n",
    "\n",
    "2. **Apply the Regularization Parameter**:\n",
    "\n",
    "\\[\n",
    "Penalty_L1 = λ * Σ(i=1 to 4) |w_i| = 0.1 * 7.0 = 0.7\\]\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **Weights**: \\( w_1 = 3.5 \\), \\( w_2 = -2.0 \\), \\( w_3 = 0.0 \\), \\( w_4 = 1.5 \\)\n",
    "- **Sum of Absolute Weights**: 7.0\n",
    "- **L1 Penalty**: 0.7 (with \\( lambda = 0.1 \\))\n",
    "\n",
    "This penalty is added to the loss function during model training. It encourages the model to have smaller weights and can force some weights to be exactly zero, effectively selecting only the most important features.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- The penalty term discourages large weights by adding a cost proportional to their absolute values.\n",
    "- It can lead to some weights being exactly zero, which simplifies the model and can improve performance on new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977d4e0",
   "metadata": {},
   "source": [
    "###\n",
    "## <u>Logistic Regression Parameters</u>\n",
    "### 1) C (Inverse of Regularization Strength):\n",
    "**Explanation:** C is a regularization parameter that controls the amount of regularization applied to the model. It is the inverse of the regularization strength, meaning that smaller values of C imply stronger regularization (to prevent overfitting), while larger values of C imply weaker regularization.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* If C=1.0 (default), the model uses the default regularization.\n",
    "* If C=0.01, the model applies stronger regularization, potentially leading to a simpler model that generalizes better.\n",
    "* If C=100, the model applies very weak regularization, allowing it to fit the training data more closely.  \n",
    "\n",
    "### 2) Penalty\n",
    "**Explanation:** The penalty parameter specifies the type of regularization to be applied. Common options are:\n",
    "\n",
    "* 'l2' (default): L2 regularization (Ridge), which penalizes the sum of squared coefficients.\n",
    "* 'l1': L1 regularization (Lasso), which penalizes the sum of the absolute values of the coefficients, leading to sparse solutions (some coefficients being zero).\n",
    "* 'elasticnet': A combination of L1 and L2 regularization.\n",
    "* 'none': No regularization applied.\n",
    "\n",
    "**Example:**\n",
    "* Use 'l1' to encourage sparsity (i.e., feature selection), useful when many features are irrelevant.\n",
    "* Use 'l2' for smoother, more generalized models\n",
    "\n",
    "### 3) Solver\n",
    "\n",
    "Explanation: The solver parameter specifies the algorithm used for optimization. Common solvers include:\n",
    "* 'liblinear': A good choice for small datasets and for L1 regularization.\n",
    "* 'saga': Handles both L1 and L2 regularization and is suitable for large datasets.\n",
    "* 'lbfgs': Suitable for large datasets, supports only L2 regularization.\n",
    "* 'newton-cg': Suitable for large datasets, supports only L2 regularization.\n",
    "\n",
    "Example:\n",
    "* If your dataset is small and you want to use L1 regularization, use 'liblinear'.\n",
    "* For large datasets and L2 regularization, use 'lbfgs'.\n",
    "\n",
    "### 4) max_iter (Maximum Iterations)\n",
    "**Explanation**: The max_iter parameter specifies the maximum number of iterations allowed for the solver to converge. The max_iter parameter comes into play when the solver (algorithm used to find the optimal parameters for the model) needs more iterations to converge to a solution.\n",
    "\n",
    "Convergence means the model has found a set of parameters that minimizes the error or cost function. \n",
    "This is important if your model struggles to converge (i.e., find the optimal solution) in a reasonable number of steps.\n",
    "\n",
    "Example:\n",
    "* If the model isn't converging, you can increase max_iter to allow more iterations.\n",
    "\n",
    "### tol (Tolerance for Stopping Criteria) \n",
    "**Explanation:** The tol parameter sets the tolerance for the stopping criterion. The solver stops when the improvement in the cost function is smaller than this threshold. A smaller tol value requires more precision (more iterations), while a larger value stops the training earlier.\n",
    "\n",
    "**Example:**\n",
    "* If the model is converging too slowly, you can increase tol to stop earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f4fc2",
   "metadata": {},
   "source": [
    "###\n",
    "## <u>SVM Parameters</u>\n",
    "### 1) C (Inverse of Regularization Strength):\n",
    "**Explanation:** C is a regularization parameter that controls the amount of regularization applied to the model. It is the inverse of the regularization strength, meaning that smaller values of C imply stronger regularization (to prevent overfitting), while larger values of C imply weaker regularization.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* If C=1.0 (default), the model uses the default regularization.\n",
    "* If C=0.01, the model applies stronger regularization, potentially leading to a simpler model that generalizes better.\n",
    "* If C=100, the model applies very weak regularization, allowing it to fit the training data more closely.  \n",
    "\n",
    "### 2) kernel (Kernel Type)\n",
    "**Explanation:** The kernel parameter defines the type of kernel function used by the SVM to transform data into a higher-dimensional space. Common kernel types include:\n",
    "* 'linear': Linear kernel (suitable for linearly separable data).\n",
    "* 'poly': Polynomial kernel (useful for non-linear data).\n",
    "* 'rbf': Radial basis function (Gaussian kernel, good for non-linear data).\n",
    "* 'sigmoid': Sigmoid kernel.\n",
    "\n",
    "**Example:**\n",
    "* If your data is linearly separable, you can use a 'linear' kernel.\n",
    "* For more complex, non-linear data, the 'rbf' (default) or 'poly' kernel might be more appropriate.\n",
    "\n",
    "### 3) degree (Degree of the Polynomial Kernel)\n",
    "**Explanation:** The degree parameter is relevant only if you're using the 'poly' (polynomial) kernel. It defines the degree of the polynomial function used in the kernel. Higher degrees make the model more complex.\n",
    "\n",
    "**Example:**\n",
    "* A degree of 2 will use a quadratic polynomial, while a degree of 3 will use a cubic polynomial.\n",
    "* The default is 3, and you can adjust it based on your data’s complexity.\n",
    "\n",
    "### 4) gamma (Kernel Coefficient)\n",
    "**Explanation:** gamma defines how far the influence of a single training example reaches, specifically in the RBF, poly, and sigmoid kernels. A low gamma means that each point’s influence is far-reaching (smoother decision boundary), while a high gamma makes the influence more localized (tighter, more complex decision boundary).\n",
    "\n",
    "**Example:**\n",
    "* A small gamma (e.g., gamma=0.01) will result in a smooth, generalized decision boundary.\n",
    "* A large gamma (e.g., gamma=10) will lead to a more complex boundary that tightly fits the training data.\n",
    "\n",
    "### 5) max_iter (Maximum Iterations)\n",
    "**Explanation**: The max_iter parameter specifies the maximum number of iterations allowed for the solver to converge. The max_iter parameter comes into play when the solver (algorithm used to find the optimal parameters for the model) needs more iterations to converge to a solution.\n",
    "\n",
    "Convergence means the model has found a set of parameters that minimizes the error or cost function. \n",
    "This is important if your model struggles to converge (i.e., find the optimal solution) in a reasonable number of steps.\n",
    "\n",
    "**Example:**\n",
    "* If the model isn't converging, you can increase max_iter to allow more iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dac399",
   "metadata": {},
   "source": [
    "###\n",
    "## <u>Cross Validation</u>\n",
    "###\n",
    "\n",
    "#### K-Fold Cross Validation\n",
    "\n",
    "This image illustrates the concept of **K-Fold Cross Validation**, a technique used in machine learning for model evaluation and validation.\n",
    "\n",
    "#### Breakdown of the Image:\n",
    "\n",
    "#### 1. All Data:\n",
    "- The dataset is first split into two parts:\n",
    "  - **Training Data**: A portion of the dataset used to train the model.\n",
    "  - **Test Data**: A separate portion reserved for final testing, kept aside from training to evaluate the model's performance on unseen data.\n",
    "\n",
    "#### 2. K-Fold Cross Validation:\n",
    "- **K = 5** in this example, meaning the training data is divided into 5 equal subsets (folds).\n",
    "- The process iterates 5 times (one for each fold) to validate the model, where:\n",
    "  - **Training on 4 folds**: Each time, 4 folds are used to train the model.\n",
    "  - **Testing on 1 fold**: The remaining fold (out of the 5) is used to test the model.\n",
    "\n",
    "#### 3. Iteration Process:\n",
    "- In each iteration, a different fold is used for testing, while the remaining 4 are used for training.\n",
    "- This ensures that each data point is used both for training and testing, allowing for a more generalized evaluation of the model.\n",
    "\n",
    "#### 4. Final Model Evaluation:\n",
    "- After completing all 5 iterations, the performance of the model is averaged over the 5 test results.\n",
    "- This approach reduces the bias that might result from a single train-test split and provides a more reliable estimate of the model's generalization capability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0077d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
